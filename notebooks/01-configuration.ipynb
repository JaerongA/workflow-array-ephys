{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To properly run the array ephys workflow, we need to properly set up the DataJoint configuration. The configuration will be saved in a file called `dj_local_conf.json` on each machine and this notebook walks you through the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up configuration in root directory of this package\n",
    "\n",
    "As a convention, we set the configuration up in the root directory of the package and always starts importing datajoint and pipeline modules from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/pu.win.princeton.edu/shans'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datajoint as dj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure database host address and credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set up the host, user and password in the `dj.config` global variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.config['database.host'] = '{YOUR_HOST}'\n",
    "dj.config['database.user'] = '{YOUR_USERNAME}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The password could be set with `dj.config['database.password'] = '{YOUR_PASSWORD}'`, but please be careful not to push your password to github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to connect to the database at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting shans@datajoint00.pni.princeton.edu:3306\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataJoint connection (connected) shans@datajoint00.pni.princeton.edu:3306"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dj.conn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure the `custom` field in `dj.config` for the Ephys element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ephys pipeline is based on the [DataJoint Array Ephys Element](https://github.com/datajoint/element-ephys). Installation of the element into the bl_pipeline requires the following configurations in the field `custom`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database prefix\n",
    "\n",
    "Giving a prefix to schema could help on the configuration of privilege settings. For example, if we set prefix `neuro_`, every schema created with the current workflow will start with `neuro_`, e.g. `neuro_lab`, `neuro_subject`, `neuro_ephys` etc.\n",
    "\n",
    "The prefix could be configurated as follows in `dj.config`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.config['custom'] = {'database.prefix': 'neuro_'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root directory for raw ephys data\n",
    "\n",
    "+ `ephys_root_data_dir` field indicates the root directory for the ephys raw data from SpikeGLX (e.g. `*imec0.ap.bin`, `*imec0.ap.meta`, `*imec0.lf.bin`, `imec0.lf.meta`)\n",
    "+ In the database, every path for the ephys raw data is relative to this root path. The benefit is that the absolute path could be configured for each machine, and when data transfer happens, we just need to change the root directory in the config file.\n",
    "+ This path is specific to each machine, as the name of drive mount could be different for each \n",
    "+ In the context of the workflow, all the paths saved into the database or saved in the config file need to be in the POSIX standards (Unix/Linux), with `/`. The path conversion for Windows machines is taken care of inside the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.config['custom']['ephys_root_data_dir'] = ['/tmp/archive/brody/RATTER/PhysData/Raw/']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root directory for kilosort 2 processed results\n",
    "\n",
    "+ `clustering_root_data_dir` field indicates the root directory for the ephys raw data from Kilosort2 (e.g. `spikes_clusters.npy`, `spikes_times.npy` etc.)\n",
    "+ In the database, every path for the kilosort output data is relative to this root path. The benefit is that the absolute path could be configured for each machine, and when data transfer happens, we just need to change the root directory in the config file.\n",
    "+ It could be the same or different from `ephys_root_data_dir`\n",
    "+ This path is specific to each machine, and here is an example of the root directory on a linux machine. In brody lab, the raw ephys data are located in the bucket server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.config['custom']['clustering_root_data_dir'] = '/mnt/bucket/labs/brody/RATTER/PhysData/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the configuration as a json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the proper configurations, we could save this as a file, either as a local json file, or a global file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.config.save_local()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local configuration file is saved as `dj_local_conf.json` in the root directory of this package `bl_pipeline_python`. Next time if you change your directory to `bl_pipeline_python` before importing datajoint and the pipeline packages, the configurations will get properly loaded.\n",
    "\n",
    "If saved globally, there will be a hidden configuration file saved in your root directory. The configuration will be loaded no matter where the directory is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dj.config.save_global()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "bl_dev",
   "language": "python",
   "name": "bl_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
