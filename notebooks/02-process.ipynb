{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run array ephys workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks you through the steps to run the ephys workflow.  \n",
    "The workflow requires neuropixels meta file and kilosort output data. To configure the paths properly, refer to [00-Set_up_the_configuration_file](./00-Set_up_the_configuration_file.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the local configuration, we will change the directory to the package root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's start by importing the relevant modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datajoint as dj\n",
    "from bl_pipeline import lab, subject, acquisition\n",
    "from bl_pipeline.ephys_element import ephys_element, probe_element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "`dj.Diagram` enables checking pipeline structure and table dependencies."
   },
   "source": [
    "+ dj.Diagram is a useful command to visualize the workflow structure and table dependencies.\n",
    "+ Major DataJoint Elements installed in the current workflow:\n",
    "    + lab\n",
    "    + subject\n",
    "    + session\n",
    "    + probe\n",
    "    + ephys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## conn.list_schemas, schema.list_tables, Diagram and describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "Two DataJoint elements `probe_element` and `ephys_element` have been installed into `bl_pipeline`"
   },
   "outputs": [],
   "source": [
    "dj.Diagram(subject.Rats) + dj.Diagram(acquisition.Sessions) + dj.Diagram(probe_element) + dj.Diagram(ephys_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestion of subjects, sessions, probes data\n",
    "Extract user-specified information from `/user_data/subjects.csv` and `/user_data/sessions.csv` and insert into corresponding tables:\n",
    "+ subject.Subject\n",
    "+ Session\n",
    "+ probe.Probe\n",
    "+ ephys.ProbeInsertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Manually ingest subjects, sessions (Probe.insert1(...))\n",
    "ingest.ingest_subjects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Introduce the automatic functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "For chornic probe insertions, `ephys_element.ProbeInsertion` directly depends on `subject.Rats`"
   },
   "source": [
    "## Ephys element starts with table `ProbeInsertion`, as a child table of `subject.Rats`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "Each entry in the table `acquisition.Sessions` describes an experimental session within a particular date."
   },
   "outputs": [],
   "source": [
    "dj.Diagram(subject.Rats) + dj.Diagram(acquisition.Sessions) + dj.Diagram(probe_element.Probe) + \\\n",
    "(dj.Diagram(ephys_element.ProbeInsertion) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ In this experiment with chronic probe insertions, the table `ephys_element.ProbeInsertion` directly depends on `subject.Rats`\n",
    "+ Each entry in `acquisition.Sessions` represents an experimental session on a particular date.\n",
    "+ Each entry in `ephys_element.EphysRecording` is for a particular probe insertion and a session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we will work on the following session throughout the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "For each combination of `acquisition.Sessions` and `ephys_element.ProbeInsertion`, there is a `ephys_element.EphysRecording`"
   },
   "outputs": [],
   "source": [
    "session_key = (acquisition.Sessions & 'session_rat=\"A256\"' & 'session_date=\"2020-09-28\"').fetch1('KEY')\n",
    "acquisition.Sessions & session_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest Probe and ProbeInsertion by ephys_element_ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.Diagram(probe_element.Probe) + acquisition.Sessions + ephys_element.EphysRecording"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A module `ephys_element_ingest` was provided to process a ephys session based on the neuropixel meta file: ingest entries into tables `Probe` and `ProbeInsertion`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bl_pipeline.ingest import ephys_element_ingest\n",
    "ephys_element_ingest.process_session(session_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, there will contents in the following tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_element.Probe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys_element.ProbeInsertion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate EphysRecording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.Diagram(acquisition.Sessions) + (dj.Diagram(probe_element.ElectrodeConfig) + 1) + \\\n",
    "ephys_element.EphysRecording + ephys_element.EphysRecording.EphysFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first argument restricts the populate to a particular subset.\n",
    "ephys_element.EphysRecording.populate(session_key, display_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate EphysRecording extracts the following information from .ap.meta file from SpikeGLX:\n",
    "\n",
    "1. **probe_element.EelectrodeConfig**: this procedure detects new ElectrodeConfig, i.e. which 384 electrodes out of the total 960 on the probe were used in this ephys session, and save the results into the table `probe_element.EelectrodeConfig`. Each entry in table `ephys_element.EphysRecording` specifies which ElectrodeConfig is used in a particular ephys session. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this ephys session we just populated, Electrodes 0-383 was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_element.ElectrodeConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_element.ElectrodeConfig.Electrode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **ephys_element.EphysRecording**: note here that it refers to a particular electrode_config identified with a hash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys_element.EphysRecording() & session_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **ephys_element.EphysRecording.EphysFile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys_element.EphysRecording.EphysFile() & session_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ClusteringTask and run/validate Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.Diagram(ephys_element.EphysRecording) + ephys_element.ClusteringParamSet + ephys_element.ClusteringTask + \\\n",
    "ephys_element.Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next major table in the ephys pipeline is the `ClusteringTask`.\n",
    "\n",
    "+ In the future release of ephys elements, we will aim to trigger Clustering within the workflow, and register an entry in `ClusteringTask` is a manual step to let the pipeline know that there is a Clustering Task to be processed.\n",
    "\n",
    "+ Currently, we have not supported the processing of Kilosort2 within the workflow. `ClusteringTask` is a place holder\n",
    "indicating a Kilosort2 clustering task is finished and the clustering results are ready for processing. \n",
    "\n",
    "+ The `ClusteringTask` table depends on the table `ClusteringParamSet`, which are the parameters of the clustering task and needed to be inserted first. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A method of the class `ClusteringParamSet` called `insert_new_params` helps on the insertion of params_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert clustering task manually\n",
    "params_ks = {\n",
    "    \"fs\": 30000,\n",
    "    \"fshigh\": 150,\n",
    "    \"minfr_goodchannels\": 0.1,\n",
    "    \"Th\": [10, 4],\n",
    "    \"lam\": 10,\n",
    "    \"AUCsplit\": 0.9,\n",
    "    \"minFR\": 0.02,\n",
    "    \"momentum\": [20, 400],\n",
    "    \"sigmaMask\": 30,\n",
    "    \"ThPr\": 8,\n",
    "    \"spkTh\": -6,\n",
    "    \"reorder\": 1,\n",
    "    \"nskip\": 25,\n",
    "    \"GPU\": 1,\n",
    "    \"Nfilt\": 1024,\n",
    "    \"nfilt_factor\": 4,\n",
    "    \"ntbuff\": 64,\n",
    "    \"whiteningRange\": 32,\n",
    "    \"nSkipCov\": 25,\n",
    "    \"scaleproc\": 200,\n",
    "    \"nPCs\": 3,\n",
    "    \"useRAM\": 0\n",
    "}\n",
    "ephys_element.ClusteringParamSet.insert_new_params(\n",
    "    'kilosort2', 0, 'Spike sorting using Kilosort2', params_ks)\n",
    "ephys_element.ClusteringParamSet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are then able to insert an entry into the `ClusteringTask` table. One important field of the table is `clustering_output_dir`, which specifies the Kilosort2 output directory for the later processing.  \n",
    "**Note**: this output dir is a relative path to be combined with `clustering_root_directory` in the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys_element.ClusteringTask.insert1(\n",
    "    dict(session_key, ratname='A256', insertion_number=0, paramset_idx=0,\n",
    "         clustering_output_dir='NP_sorted/Adrian/A256/A256_2020_09_28/A256_2020_09_28_g0/spikesort_2020_11_23_09_09_42_ks2jrc'),\n",
    "    skip_duplicates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys_element.ClusteringTask() & session_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are then able to populate the clustering results. The `Clustering` table now validates the Kilosort2 outcomes before ingesting the spike sorted results. In the future release of elements-ephys, this table will be used to trigger Kilosort2. A record in the `Clustering` indicates that Kilosort2 job is done successfully and the results are ready to be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys_element.Clustering.populate(display_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys_element.Clustering() & session_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import clustering results and manually curated results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to ingest the clustering results (spike times etc.) into the database. These clustering results are either directly from Kilosort2 or with manual curation. Both ways share the same format of files. In the element, there is a `Curation` table that saves this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.Diagram(ephys_element.ClusteringTask) + ephys_element.Clustering + ephys_element.Curation + \\\n",
    "ephys_element.CuratedClustering + ephys_element.CuratedClustering.Unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ If a manual curation was implemented, an entry needs to be manually inserted into the table `Curation`, which specifies the directory to the curated results in `curation_output_dir`.\n",
    "\n",
    "+ If we would like to process the Kilosort2 outcome directly, an entry is also needed in `Curation`. A method `create1_from_clustering_task` was provided to help this insertion. It copies the `clustering_output_dir` in `ClusteringTask` to the field `curation_output_dir` in the table `Curation` with a new `curation_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = (ephys_element.ClusteringTask & session_key).fetch1('KEY')\n",
    "ephys_element.Curation().create1_from_clustering_task(key)\n",
    "ephys_element.Curation() & session_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we could populate table `CuratedClustering`, ingesting either the output of Kilosort2 or the curated results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys_element.CuratedClustering.populate(session_key, display_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The part table `CuratedClustering.Unit` contains the spike sorted units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys_element.CuratedClustering.Unit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate LFP and spike waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two additional tables in the ephys_element that is able to get automatically processed:\n",
    "+ LFP and LFP.Electrode: By populating LFP, LFP of every other 9 electrode on the probe will be saved into table `ephys_element.LFP.Electrode` and an average LFP saved into table `ephys_element.LFP`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.Diagram(ephys_element.EphysRecording) + ephys_element.LFP + ephys_element.LFP.Electrode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys_element.LFP.populate(session_key, display_progress=True)\n",
    "ephys_element.LFP & session_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys_element.LFP.Electrode & session_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dj.Diagram(ephys_element.CuratedClustering.Unit) + ephys_element.Waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Waveform: `Waveform` table computes the average spike waveform of the channel with peak amplitudes. It takes a while to populate depending on the size of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "title": "The `probe_element.EelectrodeConfig` table conains the configuration information of the electrodes used, i.e. which 384 electrodes out of the total 960 on the probe were used in this ephys session, while the table `ephys_element.EphysRecording` specify which ElectrodeConfig is used in a particular ephys session."
   },
   "outputs": [],
   "source": [
    "ephys_element.Waveform.populate(session_key, display_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys_element.LFP.Electrode & session_key"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "bl_dev",
   "language": "python",
   "name": "bl_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
